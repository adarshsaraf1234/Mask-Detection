{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Author:-Adarsh Saraf\n",
    "Task:-Detection of Face Mask in real time.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2                                             #Importing all the dependencies \n",
    "import numpy as np\n",
    "from tensorflow.python.keras.models import load_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 1:- detect the face of the person"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_file=\"opencv_face_detector_uint8.pb\"   #path for model file\n",
    "config_file=\"opencv_face_detector.pbtxt\"      #path for the config file\n",
    "net=cv2.dnn.readNetFromTensorflow(model_file,config_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "cap=cv2.VideoCapture(0)             #start capturing through webcam\n",
    "model=load_model(\"model.h5\")        #loading the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 2:- Find your ROI \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 224, 224, 3)\n",
      "0\n",
      "(1, 224, 224, 3)\n",
      "0\n",
      "(1, 224, 224, 3)\n",
      "0\n",
      "(1, 224, 224, 3)\n",
      "0\n",
      "(1, 224, 224, 3)\n",
      "0\n",
      "(1, 224, 224, 3)\n",
      "0\n",
      "(1, 224, 224, 3)\n",
      "0\n",
      "(1, 224, 224, 3)\n",
      "1\n",
      "(1, 224, 224, 3)\n",
      "1\n",
      "(1, 224, 224, 3)\n",
      "1\n",
      "(1, 224, 224, 3)\n",
      "1\n",
      "(1, 224, 224, 3)\n",
      "1\n",
      "(1, 224, 224, 3)\n",
      "1\n",
      "(1, 224, 224, 3)\n",
      "1\n",
      "(1, 224, 224, 3)\n",
      "1\n",
      "(1, 224, 224, 3)\n",
      "1\n",
      "(1, 224, 224, 3)\n",
      "1\n",
      "(1, 224, 224, 3)\n",
      "1\n",
      "(1, 224, 224, 3)\n",
      "1\n",
      "(1, 224, 224, 3)\n",
      "1\n",
      "(1, 224, 224, 3)\n",
      "1\n",
      "(1, 224, 224, 3)\n",
      "0\n",
      "(1, 224, 224, 3)\n",
      "0\n",
      "(1, 224, 224, 3)\n",
      "0\n",
      "(1, 224, 224, 3)\n",
      "0\n",
      "(1, 224, 224, 3)\n",
      "0\n",
      "(1, 224, 224, 3)\n",
      "0\n",
      "(1, 224, 224, 3)\n",
      "0\n",
      "(1, 224, 224, 3)\n",
      "0\n",
      "(1, 224, 224, 3)\n",
      "0\n",
      "(1, 224, 224, 3)\n",
      "0\n",
      "(1, 224, 224, 3)\n",
      "0\n",
      "(1, 224, 224, 3)\n",
      "0\n",
      "(1, 224, 224, 3)\n",
      "0\n",
      "(1, 224, 224, 3)\n",
      "0\n",
      "(1, 224, 224, 3)\n",
      "0\n",
      "(1, 224, 224, 3)\n",
      "0\n",
      "(1, 224, 224, 3)\n",
      "1\n",
      "(1, 224, 224, 3)\n",
      "1\n",
      "(1, 224, 224, 3)\n",
      "1\n",
      "(1, 224, 224, 3)\n",
      "1\n",
      "(1, 224, 224, 3)\n",
      "1\n",
      "(1, 224, 224, 3)\n",
      "1\n",
      "(1, 224, 224, 3)\n",
      "1\n",
      "(1, 224, 224, 3)\n",
      "1\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'breakq' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-4-59150ee96c62>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     41\u001b[0m     \u001b[0mcv2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mimshow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Video\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mframe\u001b[0m\u001b[1;33m)\u001b[0m                \u001b[1;31m#display the ongoing video\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     42\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mcv2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwaitKey\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;36m0XFF\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mord\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'q'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 43\u001b[1;33m         \u001b[0mbreakq\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'breakq' is not defined"
     ]
    }
   ],
   "source": [
    "while cap:\n",
    "    ret,frame=cap.read()\n",
    "    frame = cv2.flip(frame, 1)      #to flip the camera or prevent lateral inversion\n",
    "    (frameHeight, frameWidth) = frame.shape[:2]    #get the frameheight and the frame\n",
    "    blob = cv2.dnn.blobFromImage(cv2.resize(frame, (300, 300)), 1, (300, 300),\n",
    "                                 (78.4263377603, 87.7689143744, 114.895847746), swapRB=False, crop=False)\n",
    "\n",
    "    net.setInput(blob)\n",
    "    detections = net.forward()       #detecting the face of the person\n",
    "    bboxes = []\n",
    "\n",
    "    conf_threshold = 0.5\n",
    "\n",
    "    for i in range(0, detections.shape[2]):\n",
    "        confidence = detections[0, 0, i, 2]          # extract the confidence associated with the detections.                                                                                 # prediction\n",
    "        if confidence < 0.5:\n",
    "            continue\n",
    "\n",
    "        box = detections[0, 0, i, 3:7] * np.array([frameWidth, frameHeight, frameWidth, frameHeight]) #creating a bounding box around the face\n",
    "        (x1, y1, x2, y2) = box.astype(\"int\")\n",
    "        text = \"{:.2f}%\".format(confidence * 100)   # draw the bounding box of the face along with the associated detections\n",
    "        cv2.rectangle(frame, (x1, y1), (x2, y2), (0, 0, 255), 2) #displaying a rectangle over the bounding box\n",
    "        cv2.putText(frame, text, (x1, y1), cv2.FONT_HERSHEY_SIMPLEX, 0.45, (0, 0, 255), 2)\n",
    "        #alpha = 1.0            # Contrast control (1.0-3.0)\n",
    "        #beta = 50              # Brightness control (0-100)\n",
    "        #frame = cv2.convertScaleAbs(frame, alpha=alpha, beta=beta)\n",
    "        roi = frame\n",
    "        roi = cv2.resize(roi, (224, 224))           # resizing ROI\n",
    "        roi = np.expand_dims(roi, axis=0)\n",
    "        print(roi.shape)\n",
    "#Step 3:- making prediction on the ROI\n",
    "        preds = model.predict(roi)          #making prediction on the ROI\n",
    "        preds = np.argmax(preds)            #returning maximum value\n",
    "        print(preds)\n",
    "        if preds == 0:                      #if 0 then person is wearing a mask\n",
    "            text = \"With_mask\"\n",
    "            cv2.putText(frame, text, (50, 50), cv2.FONT_HERSHEY_SIMPLEX, 0.60, (0,255, 0), 4)\n",
    "        elif preds == 1:\n",
    "            text = \"Without_mask\"           #if 1 then person is not wearing a mask\n",
    "            cv2.putText(frame, text, (50, 50), cv2.FONT_HERSHEY_SIMPLEX, 0.60, (255, 0, 0), 4)\n",
    "    cv2.imshow(\"Video\", frame)                #display the ongoing video\n",
    "    if cv2.waitKey((1)) % 0XFF == ord('q'):\n",
    "        breakq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
